{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbdf90c-8cdc-4741-a485-f6b74b0e1b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is the KNN algorithm?\n",
    "Ans.\n",
    "The K-Nearest Neighbors (KNN) algorithm is a type of instance-based learning where a data point is classified\n",
    "or predicted based on majority voting or averaging of its k-nearest neighbors in the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf710f41-b50d-4124-ae4e-0aaa7951c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. How do you choose the value of K in KNN?\n",
    "Ans.\n",
    "Choose the value of k in K-Nearest Neighbors (KNN) based on the complexity of the data; use small k for complex \n",
    "boundaries, large k for smoother boundaries. Consider cross-validation, the square root of data points, domain\n",
    "knowledge, and experimentation to find the optimal k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acc9a69-44b2-49cd-93de-5c528eea8223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "Ans.\n",
    "1. KNN Classifier:\n",
    "Predicts a categorical or discrete class label for a data point based on the majority class among its k-nearest\n",
    "neighbors. Output is a class label or category.\n",
    "\n",
    "2. KNN Regressor:\n",
    "Predicts a continuous numerical value for a data point by averaging the target values of its k-nearest neighbors.\n",
    "Output is a numeric value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f76711d-8043-4b57-8383-190975672975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. How do you measure the performance of KNN?\n",
    "Ans.\n",
    "For KNN classification: Use metrics like accuracy, precision, recall, F1-score, and the confusion matrix.\n",
    "\n",
    "For KNN regression: Use metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error\n",
    "(RMSE), and R-squared (RÂ²).\n",
    "\n",
    "Consider cross-validation, domain-specific metrics, and watch for overfitting or underfitting. The choice of distance\n",
    "metric and k should also be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b724db5-1aa4-46da-a6b5-053d7b500c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. What is the curse of dimensionality in KNN?\n",
    "Ans.\n",
    "The curse of dimensionality in KNN means that as we add more features to our data, the algorithm's performance can suffer.\n",
    "This happens because in high-dimensional spaces, the concept of distance between points becomes less reliable. It makes\n",
    "finding nearest neighbors computationally more demanding and can lead to less accurate predictions. To tackle this issue,\n",
    "one can choose important features, use appropriate distance measures, normalize data, and consider techniques like PCA to\n",
    "reduce the number of dimensions. These strategies help maintain the effectiveness of KNN in datasets with many features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1100ad-8c53-4d77-870e-0ddd576445e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. How do you handle missing values in KNN?\n",
    "Ans.\n",
    "To handle missing values in KNN, you can either remove data points with missing values, fill them with the mean or median\n",
    "of the feature, or use advanced imputation methods like k-Nearest Neighbors imputation, where missing values are estimated\n",
    "based on values from nearby neighbors in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fde5967-76ab-4e24-82be-2d313364574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "# which type of problem?\n",
    "Ans.\n",
    "The KNN classifier is suitable for problems where we want to predict discrete categories or labels, like classifying emails\n",
    "as spam or not spam. It predicts the majority class based on the labels of its nearest neighbors.\n",
    "\n",
    "On the other hand, the KNN regressor is better for problems involving continuous values, such as predicting house prices \n",
    "based on features like size and location. It estimates a numeric value by averaging the target values of its nearest neighbors.\n",
    "\n",
    "choose the KNN classifier for classification tasks and the KNN regressor for regression tasks, depending on whether you want to\n",
    "predict categories or continuous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de28042-0eea-4167-9ff4-462b22b901fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "# and how can these be addressed?\n",
    "Strengths of KNN:\n",
    "Simple to Understand: KNN is easy to grasp conceptually, making it accessible for beginners.\n",
    "Adaptability: It can handle various types of data and works well with both small and large datasets.\n",
    "Non-parametric: It doesn't make assumptions about the underlying data distribution, making it versatile.\n",
    "\n",
    "Weaknesses of KNN:\n",
    "Computational Cost: Predictions can be computationally expensive, especially with large datasets or high dimensions.\n",
    "Sensitivity to Noise: It is sensitive to noisy data and outliers, potentially affecting the accuracy of predictions.\n",
    "Curse of Dimensionality: Performance can degrade as the number of features increases, known as the curse of dimensionality.\n",
    "\n",
    "Addressing Weaknesses:\n",
    "Feature Scaling: Normalize or standardize features to mitigate the impact of varying scales on distance calculations.\n",
    "Dimensionality Reduction: Use techniques like Principal Component Analysis (PCA) to reduce the number of dimensions.\n",
    "Distance Metric Selection: Choose appropriate distance metrics based on the nature of the data.\n",
    "Outlier Handling: Identify and handle outliers before applying KNN to improve robustness.\n",
    "Lazy Learning: For large datasets, consider approximate or optimized algorithms to reduce computational burden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2528e2e-e372-4dc0-ae35-ea83f61fdb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "Ans.\n",
    "Euclidean Distance:\n",
    "It measures the straight-line or shortest path between two points in a multi-dimensional space. It involves summing the\n",
    "squared differences of corresponding coordinates and taking the square root. for example, Think of it as the distance a\n",
    "bird would fly between two points on a map.\n",
    "\n",
    "Manhattan Distance:\n",
    "It measures the distance between two points by the sum of the absolute differences of their coordinates. It involves \n",
    "summing the absolute differences along each dimension. for example, Imagine navigating a city grid, where you can only\n",
    "move along the streets, not taking shortcuts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf9e85a-7134-433b-b868-8c8cebfe32f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q10. What is the role of feature scaling in KNN?\n",
    "Ans.\n",
    "Feature scaling plays a crucial role in K-Nearest Neighbors (KNN) by ensuring that all features contribute equally to \n",
    "distance calculations. Without scaling, features with larger magnitudes might dominate the distance metric, leading to\n",
    "biased results. Scaling puts all features on a similar scale, allowing KNN to give equal importance to each dimension \n",
    "when determining the proximity of data points."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
